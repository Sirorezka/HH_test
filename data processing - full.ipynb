{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    " \n",
    "\n",
    "from lxml import etree\n",
    "from lxml import objectify\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n"
     ]
    }
   ],
   "source": [
    "def read_data_gz(filename_str):\n",
    "    \n",
    "    # initiate parser\n",
    "    parser = etree.XMLParser(ns_clean=True, encoding='utf-8')\n",
    "    data_train_tree = etree.parse(filename_str,parser)\n",
    "\n",
    "    # set up column names\n",
    "    col_names = ['id','update-date','industries','job-name','salary','currency','employment','schedule','description','experience','contract']\n",
    "\n",
    "    root = data_train_tree.getroot()  \n",
    "    dt_train = pd.DataFrame(columns=col_names)\n",
    "    y_train = pd.DataFrame()\n",
    "\n",
    "    # process tree\n",
    "    for k in range(len(root.getchildren())):\n",
    "        dt_row = dict()   ## for storing rows\n",
    "        dt_industr =  dict()  ## for storing industries\n",
    "        if k % 100 ==0:\n",
    "            print k\n",
    "        ##k = 0\n",
    "        b_childs = root.getchildren()[k].getchildren()\n",
    "        dt_row['id'] = root.getchildren()[0].get(\"id\")\n",
    "\n",
    "        for tree_elem in b_childs:\n",
    "            tag_val = tree_elem.tag\n",
    "            num_val = tree_elem.text\n",
    "            dt_row[tag_val] = num_val\n",
    "\n",
    "            if tree_elem.tag == 'term':\n",
    "                new_tag = tree_elem.getchildren()[0].tag\n",
    "                dt_row[new_tag] = tree_elem.getchildren()[0].text\n",
    "\n",
    "            if tree_elem.tag == 'requirement':\n",
    "                new_tag = tree_elem.getchildren()[0].tag\n",
    "                dt_row[new_tag] = tree_elem.getchildren()[0].text\n",
    "\n",
    "            if tree_elem.tag == 'industries':\n",
    "                for ind_elem in tree_elem.getchildren():\n",
    "                    dt_industr [ind_elem.text] = 1\n",
    "\n",
    "        dt_industr = pd.Series(dt_industr)\n",
    "        dt_row = pd.Series(dt_row)\n",
    "\n",
    "        ## print dt_industr\n",
    "        ## print dt_row\n",
    "\n",
    "        dt_train = dt_train.append(dt_row, ignore_index=True)\n",
    "        y_train = y_train.append(dt_industr, ignore_index=True)\n",
    "        \n",
    "    return dt_train, y_train\n",
    "    \n",
    "dt_train, y_train = read_data_gz (\"Data/train.xml.gz\")\n",
    "dt_test, y_test = read_data_gz (\"Data/test.xml.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# joining all data\n",
    "dt_all = pd.concat((dt_train,dt_test),axis=0)\n",
    "dt_all.index = range (dt_all.shape[0])\n",
    "\n",
    "print max(dt_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## removing nans from y_train\n",
    "y_train [np.isnan(y_train)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_all = dt_all.drop(['industries','requirement','term','currency','update-date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s - string salary\n",
    "def process_salary(salary_str):\n",
    "    l_bound = np.nan\n",
    "    h_bound = np.nan\n",
    "    try:\n",
    "        m = re.search(u'от ([0-9]*)', salary_str)\n",
    "        if m: \n",
    "            l_bound = float(m.group(1))\n",
    "        m = re.search(u'до ([0-9]*)', salary_str)\n",
    "        if m: \n",
    "            h_bound = float(m.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    return l_bound, h_bound  \n",
    "\n",
    "\n",
    "salary_hl = pd.DataFrame(map(process_salary, dt_all['salary']),columns=['low_sal','high_sal'])\n",
    "\n",
    "print dt_all.shape\n",
    "print salary_hl.shape\n",
    "\n",
    "dt_all = dt_all.join(salary_hl)\n",
    "dt_all = dt_all.drop('salary', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## replacing nans with approximations\n",
    "mean_rel = np.mean(dt_all['high_sal']/dt_all['low_sal'])\n",
    "\n",
    "null_rows = np.isnan(dt_all['low_sal'])\n",
    "dt_all.loc[null_rows,'low_sal'] = dt_all['high_sal'][null_rows] / mean_rel\n",
    "\n",
    "null_rows = np.isnan(dt_all['high_sal'])\n",
    "dt_all.loc[null_rows,'high_sal'] = dt_all['low_sal'][null_rows] * mean_rel\n",
    "\n",
    "\n",
    "## replacing all missing values with zeroes\n",
    "dt_all['no_sal_data'] = 0\n",
    "null_rows = np.isnan(dt_all['high_sal'])\n",
    "dt_all.loc[null_rows,'no_sal_data'] = 1\n",
    "dt_all.loc[null_rows,'low_sal'] = 0\n",
    "dt_all.loc[null_rows,'high_sal'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating one-hot features for category columns\n",
    "\n",
    "def make_bin_cols(table_data):\n",
    "   pass \n",
    "\n",
    "col_names = ['employment','schedule','experience','contract']\n",
    "dt_bin = pd.DataFrame()\n",
    "dt = dt_all[col_names]\n",
    "j = 0\n",
    "\n",
    "for j in range(dt.shape[1]):\n",
    "    for row_val in np.unique(dt.iloc[:,j]):\n",
    "        new_col = np.zeros(dt.shape[0])   \n",
    "        new_col[np.array(dt.iloc[:,j] == row_val)] = 1\n",
    "        col_name = dt.columns.values[j] + \"_\" + row_val\n",
    "        dt_bin[col_name] = new_col\n",
    "        \n",
    "dt_all = dt_all.join(dt_bin)\n",
    "dt_all = dt_all.drop(col_names, axis =1 )\n",
    "##pd.Series(np.unique(dt_all['employment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(mystr):\n",
    "    text = re.sub(u\"<.*?>\",\"\", mystr)\n",
    "    text = re.sub(u\"&quot;\",\"\", text)\n",
    "    return text\n",
    "    \n",
    "dt_all['description'] = map(clean_str, dt_all['description'])\n",
    "dt_all['job-name'] = map(clean_str, dt_all['job-name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  nltk.download(\"punkt\")\n",
    "##\n",
    "##  http://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/\n",
    "##\n",
    "\n",
    "def tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    text = re.sub(u\"[^a-zA-Z0-9а-яА-Я -]\",\"\",text)\n",
    "    text = re.sub(u\"--\",\"\",text) \n",
    "    text = re.sub(u\"($-)|( -)|(-^)\",\"\",text)\n",
    "    text = re.sub(u\" 000\",u\"000\",text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    \n",
    "    for item in tokens:\n",
    "        stems.append(stemmer.stem(item))\n",
    "    return stems\n",
    "\n",
    "\n",
    "def make_tfidf_decomposition (train_dt, test_dt, pca_size):\n",
    "\n",
    "    rus_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 5), min_df=15, stop_words = rus_stopwords)\n",
    "\n",
    "    tfs_train = tfidf.fit_transform(train_dt)\n",
    "    tfs_test  = tfidf.transform(test_dt)\n",
    "\n",
    "    pca_tf = PCA(n_components = pca_size)\n",
    "    tfs_train = pd.DataFrame(pca_tf.fit_transform (tfs_train.todense()))\n",
    "    tfs_test  = pd.DataFrame(pca_tf.transform (tfs_test.todense()))\n",
    "    \n",
    "    feature_names = pd.Series(tfidf.get_feature_names())\n",
    "\n",
    "    return tfs_train, tfs_test, feature_names\n",
    "\n",
    "# pd.set_option('display.max_rows', 1000)\n",
    "# print feature_names\n",
    "# pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descr_train, descr_test, feat_name = make_tfidf_decomposition (dt_all['description'][0:10000], dt_all['description'][10000:20000], pca_size = 200)\n",
    "\n",
    "descr_train.columns = map (lambda x: 'descr ' + (str(x)), descr_train.columns.values)\n",
    "descr_test.columns  = map (lambda x: 'descr ' + (str(x)), descr_test.columns.values)\n",
    "\n",
    "descr_all = pd.concat((descr_train,descr_test),axis=0)\n",
    "dt_all = dt_all.join(descr_all)\n",
    "\n",
    "del descr_train, descr_test, descr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_train, title_test, feat_name = make_tfidf_decomposition (dt_all['job-name'][0:10000], dt_all['job-name'][10000:20000], pca_size = 30)\n",
    "\n",
    "title_train.columns = map (lambda x: 'title ' + (str(x)), title_train.columns.values)\n",
    "title_test.columns  = map (lambda x: 'title ' + (str(x)), title_test.columns.values)\n",
    "\n",
    "title_all = pd.concat((title_train, title_test),axis=0)\n",
    "dt_all = dt_all.join(title_all)\n",
    "\n",
    "del title_train, title_test, title_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dt_all.copy(deep = True)\n",
    "print X.shape\n",
    "print dt_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## look at feature names\n",
    "##\n",
    "\n",
    "pd.set_option('display.max_rows', 3000)\n",
    "print feat_name\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = dt_all['id']\n",
    "X = X.drop(['id','job-name','description'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_PROCS = 4\n",
    "\n",
    "def unwrapLS(arg, **kwarg):\n",
    "    return CLFLS.oneFit(*arg, **kwarg)\n",
    "\n",
    "class CLFLS(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, X, y, param, final):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.param = param        \n",
    "        self.final = final\n",
    "        self.clf = []          \n",
    "    \n",
    "    def oneFit(self, i):\n",
    "        if len(set(self.y[:, i])) == 2:\n",
    "            #return SVC(C = 1e-3, probability = True).fit(self.X, self.y[:, i])\n",
    "            return LinearSVC(C = self.param).fit(self.X, self.y[:, i])\n",
    "            #return Ridge().fit(self.X, self.y[:, i])\n",
    "            #return RidgeClassifier().fit(self.X, self.y[:, i])\n",
    "            #return BernoulliNB(alpha = 0.01).fit(self.X, self.y[:, i])\n",
    "            #return LinearRegression().fit(self.X, self.y[:, i])\n",
    "            #return LogisticRegression().fit(self.X, self.y[:, i])\n",
    "            #return SGDClassifier(loss = 'perceptron', penalty = 'elasticnet', n_iter = 50).fit(self.X, self.y[:, i])\n",
    "        \n",
    "    def fit(self):\n",
    "        pool = Pool(processes = N_PROCS)\n",
    "        self.clf = pool.map(unwrapLS, zip([self] * 200, array(range(200))))\n",
    "        pool.close()\n",
    "        pool.join()            \n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for i in range(200):\n",
    "            if len(set(self.y[:, i])) == 2:\n",
    "                pred = self.clf[i].decision_function(X)\n",
    "                #pred = self.clf[i].predict(X)\n",
    "                #pred = self.clf[i].predict_proba(X)[:, 0]\n",
    "            else:\n",
    "                pred = [-10] * shape(X)[0]\n",
    "\n",
    "            predicted.append(pred)\n",
    "        del self.clf\n",
    "        gc.collect()\n",
    "        \n",
    "        return array(predicted).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "np.train(y_train == 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
